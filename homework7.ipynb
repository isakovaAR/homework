{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "newsgroups_train.target_names#категории текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.03, max_features=None, min_df=12,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}),\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 12, max_df = .03)\n",
    "vectorizer.fit(newsgroups_train.data)# сформировали сокращенный словарь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x8821 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [36:58<00:00, 43.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def algorythm (K, X, it,a, b):\n",
    "    # заведем счетчики nk,w, nd,k, nk, заполненные нулями\n",
    "    n_k_w = np.zeros((K, X.shape[1]))\n",
    "    n_d_k = np.zeros((X.shape[0], K))\n",
    "    n_k = np.zeros(K)\n",
    "    \n",
    "    #случайным образом расставим теги словам, обновим счетчики  nk,w ,  nd,k ,  nk\n",
    "    doc, word = X.nonzero()\n",
    "    tags = np.random.choice(K, len(doc))\n",
    "    \n",
    "    for w,d,t in zip(word, doc, tags):\n",
    "        n_k_w[t,w] += 1\n",
    "        n_d_k[d,t] += 1\n",
    "        n_k[t] +=1\n",
    "        \n",
    "    # пока не сойдемся к стационарному режиму:\n",
    "    #для каждого  i  от 1 до  W :\n",
    "    #ndi,zi−=1 ,  nzi,wi−=1 ,  nzi−=1 \n",
    "    #для каждого  k  от 1 до  K :\n",
    "    #вычисляем  pk=(nd,k+αk)nk,wi+βwink+βsum \n",
    "    #сэмплим новый  zi  из полученного распределения  (p1,...,pK) \n",
    "    #ndi,zi+=1 ,  nzi,wi+=1 ,  nzi+=1\n",
    "    for i in tqdm(range(it)):\n",
    "        for j in range(len(doc)):\n",
    "            t = tags[j]\n",
    "            n_k_w[t,word[j]] -= 1\n",
    "            n_d_k[doc[j], t] -=1\n",
    "            n_k[t] -=1\n",
    "            \n",
    "            p = (n_d_k[doc[j], :] + a)*(n_k_w[:,word[j]] + b[word[j]]) / \\\n",
    "            (n_k + b.sum())\n",
    "            tags[j] = np.random.choice(np.arange(K), p = p / p.sum())\n",
    "            \n",
    "            n_k_w[tags[j], word[j]] += 1\n",
    "            n_d_k[ doc[j], tags[j]] += 1\n",
    "            n_k[tags[j]] += 1\n",
    "            \n",
    "    return n_k_w, n_d_k, n_k, tags\n",
    "            \n",
    "K = 20\n",
    "n_k_w, n_d_k, n_k, tags = algorythm(K, X_train, 50, 1*np.ones(K), \\\n",
    "                                    1*np.ones(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\tappreciate\tcompany\tfolks\thappens\tlove\topinions\tposting\tsimple\tsorry\twondering\n",
      "Topic 1:\tbaseball\tgames\thockey\tleague\tplay\tplayer\tplayers\tseason\tteams\twin\n",
      "Topic 2:\tcouple\toh\tposting\tposts\treply\tsorry\tsound\tsounds\tthank\tweek\n",
      "Topic 3:\tbuild\tearth\tlaunch\tlight\tlow\tmoon\tnasa\torbit\tproject\tradio\n",
      "Topic 4:\tal\tcouldn\tdave\tdeleted\thear\tinternet\toffice\tsorry\tuucp\tuunet\n",
      "Topic 5:\t13\t17\t18\t19\t21\t22\t23\t24\t27\t40\n",
      "Topic 6:\tcountry\tcrime\tforce\tgun\tguns\tisrael\tisraeli\tlaws\trights\tweapons\n",
      "Topic 7:\tbible\tchrist\tchristian\tchristians\tchurch\tclaim\tfaith\tjesus\treligion\ttruth\n",
      "Topic 8:\tbike\tbought\tcars\tengine\thead\tmiles\tride\troad\tspeed\tturn\n",
      "Topic 9:\tcause\tcommon\tdisease\teffect\texperience\tfood\tmedical\tresults\ttreatment\tusually\n",
      "Topic 10:\tbtw\texact\thaven\thello\tmarket\tmichael\tradio\tsafety\tsomebody\tstory\n",
      "Topic 11:\tboard\tdisk\tdos\tmac\tmemory\tmonitor\tpc\tram\tsale\tvideo\n",
      "Topic 12:\tamerican\tapril\tdevelopment\thouse\tnational\tprovide\tresearch\tuniversity\twashington\twhite\n",
      "Topic 13:\tdifference\tespecially\tgets\tgoes\theat\thot\tones\twait\twater\twondering\n",
      "Topic 14:\tbob\tcompany\tdavid\tdeleted\teasy\texactly\tsea\tsingle\tstay\tthinking\n",
      "Topic 15:\tapplication\tcode\tdisplay\tfiles\tftp\tgraphics\tserver\tsun\tuser\twindow\n",
      "Topic 16:\tclass\tcouldn\tknows\tlevel\tmentioned\tposted\treply\tuniversity\tvalue\twrite\n",
      "Topic 17:\tcare\tcouple\texactly\tfigure\tfolks\thaven\tluck\tnormal\tsorry\tworth\n",
      "Topic 18:\tarmenian\tarmenians\tchildren\tkilled\tmen\ttaken\ttook\tturkish\twent\twomen\n",
      "Topic 19:\talgorithm\tchip\tclipper\tencryption\tkeys\tmessage\tnsa\tsecret\tsecure\tsecurity\n"
     ]
    }
   ],
   "source": [
    "word = np.argsort(n_k_w)[:,:-11:-1]\n",
    "for k in range(20):\n",
    "    a = np.zeros((1, X_train.shape[1]))\n",
    "    for w in word[k]:\n",
    "        a[0, w] = 1\n",
    "    print('Topic {}:\\t{}'.format(k, '\\t'.join(vectorizer.inverse_transform(a)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем следующие темы:\n",
    "1.\n",
    "2. Спорт\n",
    "3.\n",
    "4.Космос\n",
    "5.\n",
    "6.\n",
    "7.Политика, оружие\n",
    "8.Религия\n",
    "9.Машины\n",
    "10.\n",
    "11.\n",
    "12.Компьютер\n",
    "13.\n",
    "14.\n",
    "15.\n",
    "16.Hardware\n",
    "17.\n",
    "18.\n",
    "19.\n",
    "20.Криптография\n",
    "\n",
    "Все они есть в исходном датасете. Если же увеличить число иттераций до 70 и немного расширить словарь можно получить более точный список, однако это требует много времени"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
